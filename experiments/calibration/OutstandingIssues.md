# Outstanding issues

* As the model runs forward in time and the behaviour of the disease evolves, the parameterisation of the model may need to change in order to make accurate predictions. To allow for this, the model is set up to run with dynamic calibration. The model is run with ABC over 14 day intervals. After the first 14 day period the posterior estimate of the parameter values is then used as a prior for the next 14 days. However, currently, instead of the model continuing to run from day 15 in the second interval, it starts again from day 0 and continues up to day 28, and then in the next instance starts from day 0 and continues up to day 42. Given the assumption that parameter values are changing over time, this is not optimal, as we are always calibrating on the whole time period, rather than just the last 14 days. However, we also need to consider that currently we are only updating parameter values, not the state (i.e. disease) values. This means that if days 0-14 go badly (for instance if there is a Covid outbreak happening in the wrong place), then it is not possible to correct for this. Starting from day 0 each time gives a chance for these kind of issues to be corrected. 

* Implementing state updating, in addition to parameter updating, would be another theoretical addition to the model 

* Currently, the accuracy of the model is determined by comparing the number of cases over the whole of Devon each week in the model with the observations. Ideally, the model would consider the distribution of cases at the MSOA level, rather than for Devon as a whole. If the model did consider the distribution of the cases in each MSOA, then ideally it would be able to consider the cases in MSOAs that are close together. For instance, if there is an outbreak of Covid in the model in one MSOA, but in the observations this isn't present in this MSOA, but is in a neighbouring MSOA, then this shouldn't be penalised as heavily.  
  * Previously code in OpenCLWrapper.disance() allowed for comparison of case numbers in each MSOA. This code is still in opencl_runner.py so can try and adapt this (but take care as method for comparing case numbers used in this (summing numbers in disease states 1-4 each day) is not correct). 

* Is comparing cases each week a fine enough temporal resolution, or should we compare daily cases?

* The case seeding process currently weights the cases across Devon on the basis of the MSOA's risk rating (which is determined by how many people are likely to bring Covid into the area (numbers of students, young people etc)). Ideally, however, this could be based on the actual case distribution amongst MSOAs in the observations. 

* When running the model for the initial parameter calibration some of the parameters do not seem to stabilise towards a consistent distribution even after 10 populations. Why is this? May be that the value for that parameter doesn't actually influence the outcome of the model. A (theoretical) next stage could be to look at the model and why it is not sensitive to certain parameters.

* Need to also look at how parameter values change when running model with dynamic calibration over longer period - i.e. do we see a clear change in the parameterisation or not? What could this tell us about evolution of the disease?

* Still need to assess whether there is an improvement in predictive ability from running the model with Bayesian updating of parameters, compared to running a  model calibrated once with historical data. This will require running the model with the 'optimal' values from the initial model calibration script (I think)
   * Is this not just what we've already done in both the InitialModelCalibration.ipynb and RunModelWithDynamicCalibration.ipynb scripts? Is it just a case of adding     the results from this to a plot showing the results from running the model with dynamic calibration, once we've run this for a longer period (currently only ran   for a maximum of 28 days)?    
